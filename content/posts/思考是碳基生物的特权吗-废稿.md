---
title: 思考是碳基生物的特权吗？
subtitle: 
tags:
  - AI
  - 计算机
  - 哲学
related_cards: 
mentioned_books: 
date: 2025-04-27T23:41:00
categories: 
banner: 
draft: true
toc: true
slug: is-thinking-animals-privilege
---

上个月谈 AI 的时候想到一个有趣的理论，兴许可以引发一些关于「思维」的思考。

在这之前需要先补充一些计算机系统的基础知识：编程语言被编译成可执行的程序的过程，其实是在将源代码编译成机器指令的过程；机器指令定义了计算机上最基础的操作。举个例子，`x = x + 1` 这个简单的自增操作，至少需要三条机器指令来完成——将 `x` 的值写入一个寄存器；寄存器自增；将寄存器中的值写回内存。

任何计算机程序，在硬件底层，都是这样完成操作的。大部分人应该都知道计算机实际上只能操作 0 和 1，AI 也一样，大语言模型实际上也只是在不断地将 0 变成 1，将 1 变成 0 而已。

那么问题来了：**如果操作 0 和 1 的不是机器，而是猴子，甚至是人呢？**<!--more-->

这在技术上是可行的，尽管大语言模型在计算答案时所执行的机器指令可能有上亿条，但总归是有迹可循的。如果把这些机器指令全都列出来，让一个有意识的生命体花足够的时间完成这些枯燥至极的运算，再将最终的结果编码，就能得到大语言模型生成出的结果。

和经典的猴子打字机不太一样，原本猴子打字机只是在无穷和随机性中生成了确定性，但计算机算法的一个基本特性就是「有穷性」，即只要时间足够，运算是可以被完成的。

有意识的猴子和拥有智力的人，在完全没有思考的情况下，跟随一系列指令算出了一个问题的答案；由于他们操作的是对他们而言无意义的 0 和 1，他们在计算时完全不知道自己在干什么。然而，他们却得出了问题的答案。

我的问题是：这算思考吗？以及，这算是带有主观的思考吗？还是说，人工智能依托某种形式，在他们的大脑中「运行」了？

如果这算思考，甚至有主观的存在，那么，另一个问题是：这样的思考发生在计算者的脑中，还是发生在外部世界？如果思考发生在外部世界，主观也存在于外部世界，而外部世界就是客观世界，那主观和客观的界限在哪里？

或者，问题的答案可以不由思考得出，那么无意识地通过计算得出答案，就是合理的。既然不思考也可以解决问题，那思考的意义是什么？

## 答案仍然需要思考得出

先来解决最后一个问题，因为它最容易，也很关键。

先从机器指令回到更高的算法层级。大语言模型是神经网络模型，设计者在建立了神经网络的输入层、输出层、隐藏层，选定了激活函数、损失函数等等过后，通过反向传播调整模型的权重和偏值，让得出的答案更准确。其中，「准确」的标准是人定义的，这个标准仍然是人思考得出的。

训练模型的过程是让系统输出的结果更接近期望的过程，如果没有人的思考，这个「期望」也就不存在。换句话说，机器其实没有「思考」，只是在通过计算给出最接近期望的答案而已。

至少就目前看来，大语言模型还没有真正的思考能力，它只是在尽力给出符合标准和期望的答案，而标准和期望也是人在思考后定义的。

## 发生在客观世界的解答过程

大语言模型没有思考能力，这是上一小节得出的结论。基于这个结论，第二个问题「这样的思考发生在计算者的脑中，还是发生在外部世界？」就没有答案，因为问题的出发点是错的。这个过程无论发生在外部世界，还是计算者的脑中，都不是思考的过程，只是一种解答问题的算法过程。

这样的过程可以由人不思考地完成，可以由猴子完成，也当然，可以由机器完成。

第一个问题「这算思考吗？」自然也有答案了。至于「主观」，其实有些难定义，因为在英文里 Subjectivity 既可以被译为「主观性」，也可以被译为「主体性」，这两个似乎是同义词但似乎又有不同，与意识、人格、能动性有关。[^1]

不过，如果要非得出「AI 的解答过程是物质的，而非意识的（客观的而非主观的）」这个结论，还要考虑另一个问题「机器是否完全不可能产生意识？」。如果答案是肯定的，那么 AI 解答问题的过程就不是物质的，而是意识的。如果能证明这个点，我们才能认为，AI **可能**是主观的、有真实思考能力的。

## 机器能完全模拟人吗？

这个问题被很多人讨论过，甚至已经发展出「人工智能哲学」这一个哲学分支。我们先来看看有关这一学科的主要观点。

### 达特茅斯会议

1956 年，一群数学家、科学家聚在一起讨论人工智能，他们其中的一些人在如今被认为是人工智能的创始者。他们在会议上提议：[^2]

> We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in [Hanover, New Hampshire](https://en.wikipedia.org/wiki/Hanover,_New_Hampshire "Hanover, New Hampshire"). The study is to proceed on the basis of the conjecture that ==every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.== An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.

简单来说，他们提议为期两个月的研究，研究基于这样的推测进行：如果可以精确的描述学习的所有面向，或智能的任何其它特征，一个机器就可以被制造来模拟它。[^3]他们想要在研究中找到让机器使用语言、创造抽象和概念的方法。

请注意原文使用的词是 conjecture，意思是 an opinion or conclusion formed on the basis of incomplete information（基于不完整信息的观点和结论）。达特茅斯会议似乎提出了很多有意思的想法，但讨论和研究都是基于这个未被证实的假设进行的，并且没有产出什么实际性的成果。

也就是说，学习的所有面向和智能的任何其他特征，究竟能否被精确描述，能否被机器模拟，在这里还没有给出理论支撑。

### 物理符号系统

如何描述思维的过程呢？艾伦·纽厄尔与赫伯特·西蒙说：

> 物理符号系统具有充分且必要的手段进行通用智能行为。

他们认为思维其实是处理符号的过程，而只要模拟人处理符号的过程，就能用机器模拟智能。不过，这样的理论也受到了抨击：

> 尼尔斯·尼尔森（Nils Nilsson）提出了四个抨击物理符号系统假设的主要“论题”或依据。
> 1. “ ‘物理符号系统假设’缺少了符号基础”这一错误主张被认为是通用智慧行为的必要条件。
> 2. 普遍认为，**AI 需要非符号处理**（例如，联结主义架构可以提供非符号处理）。
> 3. 普遍认为，**大脑根本不是一台计算机**，目前所理解的计算并没有为智力提供合适的模型。
> 4. 最后，**有些人也相信大脑本质上是无意识的，大部分发生的事情是化学反应，人类的智慧行为类似于蚁群所显示的智慧行为**。[^4]

说回来，认为思维、学习和智能可以被描述符号系统的理论，与今天基于神经网络的人工智能并不相同，也就不适用于 ChatGPT 等大语言模型。那么，不基于物理符号系统、也不试图用精确描述的算法过程模拟智能的 AI，究竟能否模拟智能？

### 模拟到什么程度才算是有智能？？

谈论这个话题我们怎么能不谈「图灵测试」呢？

图灵测试的基本逻辑类似于「如果它看起来像鸭子、游泳像鸭子、叫声像鸭子，那么它可能就是只鸭子。」当人类无法分辨出机器和人的区别时，那这台机器和人也就没有太多区别了。

很少被人提及的是，图灵测试也受到了批评。Stuart J. Russell 和彼得·诺维格写道：

> 航空工程学的教科书并没有将领域目标定义为‘制造能像鸽子一样准确飞行的机器，直到能骗过鸽子。

这句话想表达的观点是：图灵测试只能保证机器足够「人性化」，一台通过图灵测试的机器施展的可以是彻头彻尾的骗术，而无需拥有智能。

另一个有趣的实验叫「中文屋」。我了解到这个实验是之前在 Backrooms 创作的时候，社群里有人写了一篇「欧语屋」，灵感就源于这个实验。瑟尔的中文屋假设了这样一个场景：

> 把程序写在3x5的卡片上，交给一个不会中文的普通人，把他关在房间里，让他按卡片上的指示操作。他将抄写出汉字，并通过一个插槽进出房间。从外面看，这个中文屋里似乎有个完全会说中文的聪明人。问题是：房间里有人（或任何东西）懂中文吗？也就是说，有没有任何东西具有理解的心理状态？有没有任何东西能有意识地认识到正在用中文讨论什么？这个人显然没有我们所指的意识，房子本身也没有意识，卡片当然也没有意识。于是，瑟尔的结论是：中文屋或其他任何物理符号系统都不可能有心智。



[^1]: [主体性 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/主体性)

[^2]: [Dartmouth workshop - Wikipedia](https://en.wikipedia.org/wiki/Dartmouth_workshop)

[^3]: 译文取自：[人工智能哲学 - 维基百科，自由的百科全书](https://zh.wikipedia.org/zh-cn/人工智能哲學)

[^4]: 参见：[物理符号系统 - 维基百科，自由的百科全书](https://zh.wikipedia.org/zh-cn/物理符號系統)
